# -*- coding: utf-8 -*-
"""Final_LoanProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YsfMmzPCpOgGoS0iBGxGCuGTLuZWkNK0

# Loans_modified Dataset Project

**Objective: - Create a model to analyze customers loans.**



**Data Dictionary**

| Column Name | Description |
|---|---|
| loan_id | Unique loan ID |
| gender | Gender - Male/ Female|
| married | Marital status - Yes/ No|
| dependents | Number of dependents |
| education | Education - Graduate/ Not Graduate|
| self_employed | Self-employment status - Yes/ No|
| applicant_income | Applicant's income |
| coapplicant_income | Coapplicant's income |
| loan_amount | Loan amount (thousands) |
| loan_amount_term | Term of loan (months) |
| credit_history | Credit history meets guidelines - 1/ 0|
| property_area | Area of the property - Urban/ Semi Urban/ Rural|
| loan_status | Loan approval status (target) - 1/ 0|

### Importing Python Packages, Necessary Libraries and Dataset Loading
"""

#  Ignore warning suggetions given by Python
import warnings
warnings.filterwarnings("ignore")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import xgboost as xgb # Import xgboost with the alias xgb

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score, cross_val_predict
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                          f1_score, roc_auc_score, confusion_matrix, classification_report,  roc_curve )

# Load and assign loan_datset csv file to the dataframe

df = pd.read_csv("loans_modified.csv")

"""# Part 1: Data Acquisition and Initial Exploration

These commands below are essential for performing quick assessing the structure, quality, and content of our DataFrame.

They allow us to make informed decisions about data cleaning, transformation, and analysis.
"""

df.head()  # View the first few rows to understand the data structure.

#  Get a summary of the DataFrame, including data types and non-null counts.
# This will help identify missing values.

df.info()

df.describe()   # Generate descriptive statistics (mean, std, min, max, etc.) for numerical columns.

df.shape    # Check the number of rows and columns.

df.duplicated().sum()

"""Identifying Missing Values


"""

# From df.info() we can identify missing values
# Then df.isnull().sum helps identify missing values PROGRAMMATICALLY

df.isnull().sum()     # This will return the count of missing values per column.

"""# Part 2: Exploratory Data Analysis (EDA)

**Understand Target Variable(loan_status)**

To analyze the distribution of the loan_status column and determine whether it is balanced or imbalanced , **we use both descriptive statistics and visualizations**.


- loan_status is encoded as:

  1 = Approved

  0 = Rejected

- The mean is ~0.718 , indicating that about **71.8% of the loans were approved , and 28.2% were rejected**.
"""

# To view the actual counts and percentage programmatically

df['loan_status'].value_counts(dropna=True)  # dropna=True means don't include count of NAN

# Alternatively in percentage
df['loan_status'].value_counts(normalize=True) * 100

#  Visualizing the Target variable(loan_status) Distribution

sns.countplot(x='loan_status', data=df)       # Show the counts of observations in each categorical bin using bars.
plt.title('Loan Status Distribution')
plt.xlabel('Loan Approved(1 = Yes, 0 = No)')
plt.ylabel('Count')
plt.show()

'''This chart below show a higher bar for 1.0 , confirms the imbalance.'''

"""**Gender**"""

print("Count of Male and Female Applicants:")
print(df['gender'].value_counts(dropna=True))
df['gender'].value_counts(normalize=True) * 100

# This distribution indicates a class imbalance.

"""**Univariate Analysis**"""

# For numerical features (applicant_income, coapplicant_income, loan_amount, loan_amount_term):
# Plot histograms or box plots to understand their distributions and identify outliers.

selected_cols = ['applicant_income', 'coapplicant_income', 'loan_amount', 'loan_amount_term']
df[selected_cols].hist(figsize=(12,8), edgecolor='black')
plt.tight_layout()
plt.show()

"""- applicant_income and coapplicant_income

Both are heavily right-skewed with most values are concentrated below 10,000 , but some go as high as 80,000 (applicant) and 40,000 (coapplicant).
This wide range and sharp skew indicate the presence of extreme income outliers .

- loan_amount

Also right-skewed. Most loan amounts are between 50–250 , with fewer loans in the high range (eg, above 400).Indicates that **median to moderate loans are most common**, but there are a few very large loans .

- loan_amount_term

Most values are clustered around 360 months (30 years) — the standard mortgage term.
Very few loans have terms significantly different from that (like 12, 84, 180).
This suggests that loan term is not normally distributed , but categorically dominated by a common value .
"""

# Shows the distribution and outliers for each column.

selected_cols = ['applicant_income', 'coapplicant_income', 'loan_amount', 'loan_amount_term']
df[selected_cols].plot.box(figsize=(10, 6))
plt.title("Boxplot for Selected Columns")
plt.ylabel("Value")
plt.show()

"""Observations:

- applicant_income and coapplicant_income have many extreme outliers above the upper whisker.

- loan_amount has some mild outliers, but the spread is tighter than incomes.

- loan_amount_termis fairly compact but does show a few non-360 values as outliers.
"""

# Bar Charts for Categorical Features
# For categorical features (gender, married, dependents, education, self_employed, credit_history, property_area):
# Plot bar charts to visualize the frequency of each category.


categorical_cols = ['gender', 'married', 'dependents', 'education', 'self_employed', 'property_area']

for col in categorical_cols:
    df[col].value_counts(dropna=False).plot(kind='bar', edgecolor='black')
    plt.title(f'Count of {col}')
    plt.xlabel(col)
    plt.ylabel('Frequency')
    plt.show()

""" **Bivariate Analysis: Categorical vs. loan_status (Stacked Bar Charts)**"""

target = 'loan_status'
for col in categorical_cols:
    ct = pd.crosstab(df[col], df[target])
    ct.plot(kind='bar', stacked=True, edgecolor='black')
    plt.title(f'{col} vs {target}')
    plt.xlabel(col)
    plt.ylabel('Count')
    plt.legend(title=target)
    plt.show()

"""Relationship btw credit_history and loan_status"""

df.groupby('credit_history')['loan_status'].mean()

# This shows approval 80 % by credit history.

"""**Correlation Matrix & Heatmap**"""

# Calculate the correlation matrix (df.corr()) and visualize it using a heatmap to identify relationships between numerical features

# Get a list of all numeric columns automatically
num_cols = df.select_dtypes(include='number').columns.tolist()
corr = df[num_cols].corr()

plt.figure(figsize=(7, 5))
sns.heatmap(corr, annot=True, cmap='coolwarm', center=0)
plt.title('Correlation Matrix')
plt.show()

"""Key Insights:

1. credit_history is the strongest predictor of loan_status (+0.48). This means applicants with a positive credit history are much more likely to be approved.

 2. Weak or negative correlations withloan_status

| Feature              | Correlation with `loan_status` | Insight                                                                                                                      |
| -------------------- | ------------------------------ | ---------------------------------------------------------------------------------------------------------------------------- |
| `applicant_income`   | -0.026                         | Surprisingly, weak **negative** correlation. High income ≠ guaranteed approval. Possibly due to outliers or credit defaults. |
| `coapplicant_income` | -0.077                         | Also weak and slightly negative. May suggest some high-income co-applicants were denied.                                     |
| `loan_amount`        | -0.068                         | Weak negative — higher loan amounts may slightly reduce approval chances. Risk-averse behavior.                              |
| `loan_amount_term`   | -0.0062                        | Almost zero. Loan term doesn’t significantly influence approval.                                                             |

# Part 3: Data Preprocessing
"""

# Create a copy of the DataFrame to perform preprocessing steps
df_processed = df.copy()

# Drop 'loan_id' as it's a unique identifier and not useful for modeling

df_processed = df_processed.drop('loan_id', axis=1)
print("\nDropped 'loan_id' column.")

""" **3.1 Handle  'Dependents'  Column and duplicates**

"""

# Replace '3+' with '3' and convert datatype from object to numeric

df_processed['dependents'] = df_processed['dependents'].replace('3+', '3').astype(float)
print("Handled '3+' in 'dependents' and converted to numeric.")

# Drop or handle duplicates rows from the data frame before handling missing values
# This ensures we do not use information from the duplicate rows to fill in missing values.
# in order to avoid slight bias or inaccurate representation of the data

df_processed.drop_duplicates(inplace=True)

"""**3.2 Handle Missing Values (Imputation)**

From df.info(), we know and identified which columns have missing values.

Strategies:
- Dropping missing values: If only a small fraction of data is missing, you can remove the rows or columns with missing values.
- The most common and generally recommended approach for missing values in the target variable(loan_status) is to **remove the rows where the target is missing because its a supervised learning requirement, to avoids introducing bias ( model's training and evaluation) and will not significantly impact the statistical power of our analysis.**

- For Numerical: Impute with mean, median, or mode.

- For Categorical: Impute with mode or a new category like "Unknown."


"""

# Drop rows where 'loan_status' is NaN.
df_processed.dropna(subset=['loan_status'], inplace=True)

print("\nMissing values in 'loan_status' after handling:")
print(df_processed['loan_status'].isnull().sum())

# Categorical columns (impute with mode).

categorical_cols = ['gender', 'married', 'self_employed', 'education', 'property_area']
for col in categorical_cols:
  df_processed[col].fillna(df_processed[col].mode()[0],inplace=True)   # This line fills missing values with the most frequent category — known as the mode .

# Impute numerical columns with the median
# Median is a robust choice especially with skewed data like income
# You can impute all those numerical columns collectively using a loop. Here’s a compact way to do it:

num_cols = ['applicant_income', 'coapplicant_income', 'loan_amount', 'loan_amount_term', 'dependents']

for col in num_cols:
  df_processed[col].fillna(df_processed[col].median(), inplace=True)

# Fill missing 'credit_history' values with the most common value (mode)
df_processed['credit_history'].fillna(df_processed['credit_history'].mode()[0], inplace=True)

print("\n--- Missing Values After Imputation ---")
print(df_processed.isnull().sum())

print(f"New DataFrame shape after dropping rows with missing 'loan_status': {df_processed.info()}")

""" **3.3- Handling Outliers**

An outlier is typically defined as a data point that significantly deviates from other observations in a dataset. This concept primarily applies to continuous or numerical data where values can vary along a scale.

For financial/loan data, using the Log transformation is often more appropriate since large values can be legitimate.

✅ Why Log transforming:
- Log transformation handles right-skewed distributions (common with income/loan data), making the data more normal-like , which helps linear models perform better.



"""

# Outlier Handling: Apply log transformation to skewed income and loan columns

# Specify columns likely to be highly skewed
skewed_numerical_cols = ['applicant_income', 'coapplicant_income', 'loan_amount']

print("\n--- Outlier Handling: Applying Log Transformation ---")

for col in skewed_numerical_cols:
    # Only apply log1p if column exists and all values are non-negative
    if col in df_processed.columns and (df_processed[col] >= 0).all():
        df_processed[f'{col}_log'] = np.log1p(df_processed[col])  # log1p handles zeros safely
        print(f"Created '{col}_log' using log1p transformation.")
    else:
        print(f"Skipping log transformation for '{col}': negative values or column missing.")


# For modeling, we use the transformed columns and drop the originals
df_processed.drop(columns=skewed_numerical_cols, inplace=True)        # Remove originals

# Rename new columns to original names for convenience
# Rename each '_log' column to its original name
for col in skewed_numerical_cols:
    df_processed[col] = df_processed.pop(f'{col}_log')
print("Replaced original skewed numerical columns with their log-transformed versions.")

# Visualize distributions after log transformation
print("\nDistributions after Log Transformation (first 3 columns are now log-transformed):")

plt.figure(figsize=(15, 5))
for i, col in enumerate(skewed_numerical_cols):
    plt.subplot(1, len(skewed_numerical_cols), i + 1)
    sns.histplot(df_processed[col], kde=True)
    plt.title(f'Log-Transformed {col}')
plt.tight_layout()
plt.show()

"""**3.4 Categorical Encoding**"""

# Step 1: List the categorical columns that needs encoding (excluding the target column)
cols_to_encode = ['gender', 'married', 'education', 'self_employed', 'property_area']

# Step 2: Apply one-hot encoding to these columns
# This creates a new column for each category (except the first, to prevent multicollinearity)
df_processed = pd.get_dummies(df_processed, columns=cols_to_encode, drop_first=True)

# Step 3: Display confirmation and the new column structure
print("\nOne-Hot Encoding applied to selected categorical features.")
print("Final DataFrame columns after One-Hot Encoding :")
print(df_processed.columns.tolist())                      # help to verify the transformation.

"""**3.5 Feature Engineering**

This is an optional step but recommended.
"""

# 1. Create a 'Total_Income' feature
# Combine applicant and coapplicant incomes into a single total income feature
df_processed['total_income'] = df_processed['applicant_income'] + df_processed['coapplicant_income']
print("Created new feature: 'total_income'.")


# 2. Create 'Loan_Amount_Per_Income'(loan_to_income_ratio)
# Calculate the ratio of loan amount to total income (helps in assessing affordability)
# Add a tiny number to total_income to prevent division by zero
df_processed['loan_amount_per_income'] = df_processed['loan_amount'] / (df_processed['total_income'] + 1e-6)
print("Created new feature: 'loan_amount_per_income'.")

"""Rationale:
1. Why create total_income?
- Many applicants have both an applicant and a coapplicant bringing in income.
- Lenders care about household income, not just one person's.
- Combining these gives a better indicator of the household's true financial capacity.
- This single feature can capture whether someone is applying with help (coapplicant) or not.

2. Why create loan_to_income_ratio? (or loan_amount_per_income)

- This feature directly measures the burden the loan would place on the applicant(s).
- A higher ratio = the loan is a larger portion of earnings → potentially harder to repay.
- Used in real-world lending as the "debt-to-income ratio"—a key predictor of loan approval/repayment ability.
- It helps identify applicants who may struggle to repay or are financially stretched.

Summary table:

| Feature | What it tells you | Why it's useful for prediction |
|-----------------|-----------------------------------------|----------------------------------------------|
| total_income | Total borrower income | Captures full repayment capacity |
| loan_to_income_ratio | Loan burden relative to total income | Directly connected to risk of default ie  someone with low income (even with a coapplicant) and a big loan is higher risk.|

# Part 4 : Model Training and Evalution

**4.1 - Splitting Data: Training vs. Testing**

Before we can evaluate the model, we need to make sure we’re testing it on data it hasn’t seen before. This is why we split the data into two parts:
training and testing datasets.

*   **Training set**: The data the model uses to learn

*   **Testing set**: The data we use to see how well the model performs on new, unseen examples.

This helps us understand if the model has learned the underlying patterns in the data or if it’s just overfitting (i.e., memorizing specific details from the training data). Testing on a separate test set gives a more realistic view of how the model will perform in the real world when applied to new data.
"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Step 1: Define features (X) and the target variable (y)
X = df_processed.drop('loan_status', axis=1)  # Features
y = df_processed['loan_status']               # Target


# Step 2: Splitting the dataset into training and test sets

X_train, X_test, y_train, y_test = train_test_split(
                                      X, y, test_size=0.2, random_state=42, stratify=y
                                      )

scaler = StandardScaler()  # intialize the StandardScalar
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""**4.2 : Model selection and Evaluation technique**

**Trying Logistic Regression for the following reasons**:

- Binary classification problem - Loan approval is typically yes/no (approved/denied)

- Interpretability - Banks need to explain why loans are approved/denied for regulatory compliance

- Probability outputs - Gives the probability of approval, not just a yes/no decision

- Handles mixed data types - Works well with the mix of numerical (income, loan amount) and categorical (gender, education) features

- Industry standard - Widely used and trusted in financial institutions
"""

# Initialize the Logistic Regression model with SMOTE
log_reg_model = LogisticRegression(random_state=42, solver='liblinear') # liblinear is good for small datasets)

# Train the model
log_reg_model.fit(X_train, y_train)

print("Logistic Regression model trained.")

# Make predictions
y_pred_log_reg = log_reg_model.predict(X_test)
y_prob_log_reg = log_reg_model.predict_proba(X_test)[:, 1] # Get Probabilities for ROC AUC

print("\nLogistic Regression Metrics:")

# Evaluate the model's performance on the original test set
accuracy = accuracy_score(y_test, y_pred_log_reg)
precision = precision_score(y_test, y_pred_log_reg)
recall = recall_score(y_test, y_pred_log_reg)
f1 = f1_score(y_test, y_pred_log_reg)
auc_roc = roc_auc_score(y_test, y_prob_log_reg)

# Print the evaluation metrics
print("\nModel Performance on Test Data (Logistic Regression):")
print(f"Accuracy: {accuracy:.3f}")
print(f"Precision: {precision:.3f}")
print(f"Recall: {recall:.3f}")
print(f"F1-score: {f1:.3f}")
print(f"AUC-ROC Score: {auc_roc:.3f}")

print("\nConfusion Matrix (Logistic Regression):")
print(confusion_matrix(y_test, y_pred_log_reg))

# Print detailed performance per loan_status class
print("\nClassification Report:\n")
# Define target_names
target_names = ['Rejected', 'Approved']
print(classification_report(y_test, y_pred_log_reg, target_names=target_names))

"""**Trying Random Forest classifier model for the following reasons**:


*   it can automatically reduces overfitting.
*  Works well even if our features are complicated or mixed

*  Handles multiclass problems easily


"""

print("\n--- Training and Evaluating Random Forest Classifier Model ---")
# Random Forest is an ensemble method, generally robust and performs well.
rf_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')
# class_weight='balanced' can help with imbalanced datasets by giving more weight to the minority class.

rf_model.fit(X_train, y_train)    # Train the rf_model on the training set

# Make predictions on the test set
y_pred_rf = rf_model.predict(X_test)
y_prob_rf = rf_model.predict_proba(X_test)[:, 1]  # Get Probabilities for ROC AUC

print("\nRandom Forest Metrics:")
print(f"Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}")
print(f"Precision: {precision_score(y_test, y_pred_rf):.4f}")
print(f"Recall: {recall_score(y_test, y_pred_rf):.4f}")
print(f"F1-Score: {f1_score(y_test, y_pred_rf):.4f}")
print(f"ROC AUC: {roc_auc_score(y_test, y_prob_rf):.4f}")


# Print detailed performance per loan_status names
print("\nClassification Report:\n")
target_names = ['Rejected', 'Approved']   # Define target_names
print(classification_report(y_test, y_pred_rf, target_names=target_names))

# --- Confusion Matrix ---
print("\nConfusion Matrix:")
# Calculate the confusion matrix
cm = confusion_matrix(y_test, y_pred_rf)

# Create a heatmap for the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Predicted Rejected', 'Predicted Approved'],
            yticklabels=['Actual Rejected', 'Actual Approved'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix - Random Forest Model')
plt.show()

"""**Trying XGBoost classifier model for the following reasons**:

- Handles missing values internally.

- Deals well with skewed distributions and outliers .

- Robust with categorical-like numerical features (after label encoding).

- Has built-in regularization to avoid overfitting.

- Handles imbalance using scale_pos_weight.

It often outperforms logistic regression or decision trees for structured tabular data like this.
"""

# Initialize the XGBoost model with specific hyperparameters
xgb_model = XGBClassifier(
    n_estimators=100,                                              # Number of trees in the ensemble
    learning_rate=0.1,                                             # Step size shrinkage (how much each tree contributes)
    max_depth=4,                                                   # Maximum depth of trees
    subsample=0.8,                                                 # Fraction of samples used per tree (for bagging, helps prevent overfitting)
    colsample_bytree=0.8,                                          # Fraction of features to use per tree (helps prevent overfitting)
    scale_pos_weight=(y_train == 0).sum() / (y_train == 1).sum(),  # Balances classes if imbalanced
    use_label_encoder=False,                                       # Avoid a warning in newer versions of xgboost
    eval_metric='logloss',                                         # Evaluation metric for model
    random_state=42                                                # Set random seed for reproducibility
)

# Train (fit) the model on the training data
xgb_model.fit(X_train, y_train)

# Make predictions on test data
y_pred = xgb_model.predict(X_test)
y_prob_xgb = xgb_model.predict_proba(X_test)[:, 1]

print("ROC-AUC Score:", roc_auc_score(y_test, y_prob_xgb))

# Print confusion matrix to show counts of true/false positives/negatives
print(confusion_matrix(y_test, y_pred))

# Print detailed classification metrics (precision, recall, f1-score, etc.)
print(classification_report(y_test, y_pred))

# Visualize Feature Importance

xgb_model.get_booster().feature_names = X.columns.tolist()
xgb.plot_importance(xgb_model)
plt.show()

"""# --- Visualize ROC Curve of the three models used ---"""

# --- Visualize ROC Curve ---
from sklearn.metrics import roc_curve # Import roc_curve

plt.figure(figsize=(8, 6))        # Start a new figure with specified size

# Get False positve rate(FPR), true positve rate(TPR) for Logistic Regression
fpr_log_reg, tpr_log_reg, _ = roc_curve(y_test, y_prob_log_reg)

# Get FPR, TPR for Random Forest
fpr_rf, tpr_rf, _ = roc_curve(y_test, y_prob_rf)

# Get FPR and TPR for XGBoost
fpr_xgb, tpr_xgb, _ = roc_curve(y_test, y_prob_xgb)

# Plot ROC curve for Logistic Regression with AUC in the label
plt.plot(
    fpr_log_reg,
    tpr_log_reg,
    label=f'Logistic Regression (AUC = {roc_auc_score(y_test, y_prob_log_reg):.2f})'
)

# Plot ROC curve for Random Forest with AUC in the label
plt.plot(
    fpr_rf,
    tpr_rf,
    label=f'Random Forest (AUC = {roc_auc_score(y_test, y_prob_rf):.2f})'
)

# XGBoost ROC
plt.plot(
    fpr_xgb,
    tpr_xgb,
    label=f'XGBoost (AUC = {roc_auc_score(y_test, y_prob_xgb):.2f})'
)

plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')  # Plot a diagonal line for the random classifier baseline
plt.xlabel('False Positive Rate')             # Set X-axis label
plt.ylabel('True Positive Rate')              # Set Y-axis label
plt.title('ROC Curve')                        # Set the plot title
plt.legend()                                  # Show the legend (labels for each curve)
plt.grid(True)                                # Add gridlines for easier visualization
plt.show()                                    # Display the final plot

"""Project Highlights

Objective: Develop a predictive model to automate and optimize the loan approval process, aiming to balance the bank's profitability and risk management.

Data Preparation:

Handled missing values, treated outliers through log transformation for skewed financial features, and engineered new features like total_income and loan_amount_per_income.

Model Performance: Evaluated Logistic Regression, Random Forest, and XGBoost classifiers.

- Random Forest emerged as the top performer with the highest ROC AUC (0.85) and a good balance of precision and recall for both loan statuses.

- Logistic Regression performed very well for identifying 'Approved' loans (Recall=0.97), but struggled more with identifying 'Rejected' loans (Recall=0.48).

- XGBoost, surprisingly, performed slightly less effectively than the other two on this dataset based on AUC (0.79), possibly due to default hyperparameters or dataset characteristics.

Key Predictors:

**loan_amount_per_income, applicant_income, and loan_amount** were identified as the most influential factors in predicting loan approval, highlighting the importance of an applicant's financial capacity relative to the loan burden. Credit_history, while important, showed less relative importance in the XGBoost model compared to these income-related features.
"""